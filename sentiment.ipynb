{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the necessary imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchtext\n",
    "# from torchtext.data import Field, LabelField\n",
    "# from torchtext.utils.data import TabularDataset\n",
    "# from torchtext.utils.data import Iterator, BucketIterator\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "manual_seed = 572\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## CNN's for text classification\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are essentially a special case of a normal feed forward network, instead of being \"densely\" connected (note you may see people refer to Feed Forward networks as \"dense layers\"), nodes in CNNs connect to a smaller set of nodes, defined by the \"filters size\" of the network. CNNs thus have a smaller local windows to look at data, but make up for it by generally using many additional filters, which might be able to learn different aspects of the data. These networks turn out to be extremely useful for processing images, audio, and anything with some sort of spatial properties to the data.\n",
    "\n",
    "It turns out they can also be used for any sentence classification task. Words in a sentence it turns out have a sort of 1D spatial ordering, which means some classification tasks can benefit from this CNNs ability to operate over the length of the sequence. In addition, because of the sparsity of the connections, you end up being able to make much smaller networks that retain a great deal of power.\n",
    "\n",
    "#### Pytorch 1D CNNs and max-pooling\n",
    "\n",
    "Here's a quick example of how these networks function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0115, 0.1652, 0.0000, 0.0210, 0.1613, 0.3903, 0.0000, 0.1766,\n",
      "          0.1741, 0.2820],\n",
      "         [0.0000, 0.0000, 0.0083, 0.1430, 0.0000, 0.0000, 0.2834, 0.0585,\n",
      "          0.2041, 0.0000],\n",
      "         [0.0456, 0.4565, 0.1949, 0.3690, 0.0888, 0.2262, 0.0732, 0.5068,\n",
      "          0.5604, 0.3049],\n",
      "         [0.0000, 0.0000, 0.0170, 0.1959, 0.0000, 0.0990, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2387, 0.2856, 0.0000, 0.0000, 0.2042, 0.3228, 0.0630, 0.0000,\n",
      "          0.0000, 0.2929],\n",
      "         [0.0556, 0.2714, 0.0000, 0.0000, 0.0000, 0.1017, 0.1376, 0.1463,\n",
      "          0.0000, 0.0000],\n",
      "         [0.1951, 0.6214, 0.0341, 0.6379, 0.3405, 0.5166, 0.4090, 0.2046,\n",
      "          0.3178, 0.5490],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.2466]]], grad_fn=<ReluBackward0>)\n",
      "Take the highest value in each window using max pool\n",
      "tensor([[[0.1652, 0.1613, 0.3903, 0.2820],\n",
      "         [0.0000, 0.1430, 0.2834, 0.2041],\n",
      "         [0.4565, 0.3690, 0.5068, 0.5604],\n",
      "         [0.0000, 0.1959, 0.0990, 0.0000]],\n",
      "\n",
      "        [[0.2856, 0.2042, 0.3228, 0.2929],\n",
      "         [0.2714, 0.0000, 0.1463, 0.0000],\n",
      "         [0.6214, 0.6379, 0.5166, 0.5490],\n",
      "         [0.0000, 0.0000, 0.0000, 0.2466]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.rand((2,5,10))   # batch size 2 with length 10 and 5 dim embedding.\n",
    "\n",
    "in_dim = 5\n",
    "filters = 4\n",
    "\n",
    "cnn1d = nn.Conv1d(in_dim,filters,kernel_size=3,padding=1) \n",
    "max_pool = nn.MaxPool1d(kernel_size=3, padding=1)\n",
    "activation = nn.ReLU()\n",
    "x = cnn1d(x)\n",
    "x = activation(x)\n",
    "print(x)\n",
    "print(\"Take the highest value in each window using max pool\")\n",
    "print(max_pool(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've used a 1D CNN and max pooling to \"summarize\" our data, boiling a length 10 series to only 4 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1d.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "param_count = 0\n",
    "for layer in cnn1d.parameters():\n",
    "    param_count += layer.numel()\n",
    "print(param_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first layer there's an embedding of length 5 which is passed through a CNN layer with kernel size of 3 and there are 4 such filters. Thus we get 5 x 4 x3 = 60 parameters. Finally there are 4 bias values, one for each filter. Hence the total number of parameters are 64.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size through CNNs\n",
    "\n",
    "CNNs can be a little tricky because as the data passes through them the dimensions might change based on number of filters, padding, and two other things ,stride and dilation. MaxPooling also quickly can decrease the size of the data. This is useful especially as a way to \"feature extract\" or \"dimensionality reduction\" but it's important to make sure we get the right output dimensions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test your answer by making a random tensor of the appropriate size, \n",
    "## passing it through your proposed CNN+Maxpool and printing out the final size.\n",
    "\n",
    "x = torch.rand((10,25,30))   # batch size 10 with length 30 and 25 dim embedding.\n",
    "\n",
    "in_dim = 25\n",
    "filters = 5\n",
    "\n",
    "ks = 3\n",
    "\n",
    "cnn1d = nn.Conv1d(in_dim,filters,kernel_size=ks,padding=ks//2) \n",
    "max_pool = nn.MaxPool1d(kernel_size=3, padding=1)\n",
    "activation = nn.ReLU()\n",
    "x = cnn1d(x)\n",
    "x = activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 30])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = max_pool(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D CNN Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "# define the white space tokenizer to get tokens\n",
    "def tokenize_en(tweet):\n",
    "    \"\"\"\n",
    "    Tokenizes English tweet from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return tweet.strip().split()\n",
    "\n",
    "# define the TorchText's fields\n",
    "TEXT = Field(sequential=True, tokenize=tokenize_en, lower=True)\n",
    "LABEL = Field(sequential=False, unk_token = None)\n",
    "\n",
    "\n",
    "train, val, test = TabularDataset.splits(\n",
    "    path=\"./data/sentiment-twitter-2016-task4/\", # the root directory where the data lies\n",
    "    train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n",
    "    format='tsv',\n",
    "    skip_header=False, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "    fields=[('tweet', TEXT), ('label', LABEL)])\n",
    "\n",
    "TEXT.build_vocab(train, min_freq=3) # builds vocabulary based on all the words that occur at least twice in the training set\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(64,64,64),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=True\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = len(TEXT.vocab.stoi)\n",
    "LABEL_SIZE = len(LABEL.vocab.stoi)\n",
    "\n",
    "WORD_VEC_SIZE=300\n",
    "# Note, the parameters to Embedding class below are:\n",
    "# num_embeddings (int): size of the dictionary of embeddings\n",
    "# embedding_dim (int): the size of each embedding vector\n",
    "# For more details on Embedding class, see: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/sparse.py\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "  \n",
    "    def __init__(self, layer_num, filtersize, filters,nonlin, output_size, VOCAB_SIZE,  WORD_VEC_SIZE):  #feel free to add additional parameters\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE, sparse=True)\n",
    "        self.embedding.weight.data.normal_(0.0,0.05)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.nonlin = nonlin\n",
    "        for i in range(layer_num):\n",
    "            if i == 0:\n",
    "            # YOUR CODE HERE    (FIRST LAYER CNN CODE)\n",
    "                self.layers.append(nn.Conv1d(WORD_VEC_SIZE, filters, kernel_size=filtersize, padding=filtersize//2))\n",
    "            else: \n",
    "                # YOUR CODE HERE    (LATER LAYER CNN CODE)\n",
    "                self.layers.append(nn.Conv1d(filters, filters,kernel_size=filtersize,padding=filtersize//2))\n",
    "            self.layers.append(self.nonlin)\n",
    "                    \n",
    "        self.max_layer = nn.AdaptiveMaxPool1d(1)\n",
    "        self.output = nn.Linear(filters, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        # PASS x THROUGH EMBEDDING (CHECK DIMENSIONS!!!):\n",
    "        x = self.embedding(x) # Ouput: (L, N, C)\n",
    "        # ENSURE DIMs CORRECT FOR CNN:\n",
    "        x = x.permute(1, 2, 0) # CNN needs input (N,C,L)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.max_layer(x).squeeze(dim=-1)\n",
    "        x =self.softmax(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(loader,model,criterion,optimizer,device):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader,model,criterion,device):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Example run through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (embedding): Embedding(3330, 300, sparse=True)\n",
       "  (layers): ModuleList(\n",
       "    (0): Conv1d(300, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (3): ReLU()\n",
       "    (4): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (5): ReLU()\n",
       "    (6): Conv1d(10, 10, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (nonlin): ReLU()\n",
       "  (max_layer): AdaptiveMaxPool1d(output_size=1)\n",
       "  (output): Linear(in_features=10, out_features=3, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet(4, 3, 10, nn.ReLU(), LABEL_SIZE, VOCAB_SIZE, WORD_VEC_SIZE)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a batch of data\n",
    "\n",
    "for batch in train_iter:\n",
    "    tweets = batch.tweet\n",
    "    labels = batch.label\n",
    "    break  #we use first batch as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3135, -0.8572, -1.1816],\n",
       "        [-1.3131, -0.8567, -1.1827],\n",
       "        [-1.3132, -0.8567, -1.1826],\n",
       "        [-1.3135, -0.8567, -1.1823],\n",
       "        [-1.3136, -0.8566, -1.1825],\n",
       "        [-1.3135, -0.8565, -1.1827],\n",
       "        [-1.3136, -0.8569, -1.1820],\n",
       "        [-1.3133, -0.8575, -1.1814],\n",
       "        [-1.3135, -0.8572, -1.1816],\n",
       "        [-1.3139, -0.8575, -1.1809],\n",
       "        [-1.3134, -0.8571, -1.1818],\n",
       "        [-1.3137, -0.8569, -1.1819],\n",
       "        [-1.3133, -0.8571, -1.1819],\n",
       "        [-1.3137, -0.8576, -1.1809],\n",
       "        [-1.3135, -0.8568, -1.1822],\n",
       "        [-1.3137, -0.8573, -1.1814],\n",
       "        [-1.3135, -0.8571, -1.1817],\n",
       "        [-1.3137, -0.8573, -1.1814],\n",
       "        [-1.3137, -0.8572, -1.1814],\n",
       "        [-1.3135, -0.8566, -1.1824],\n",
       "        [-1.3129, -0.8571, -1.1823],\n",
       "        [-1.3136, -0.8573, -1.1815],\n",
       "        [-1.3137, -0.8573, -1.1813],\n",
       "        [-1.3135, -0.8569, -1.1820],\n",
       "        [-1.3134, -0.8572, -1.1817],\n",
       "        [-1.3136, -0.8573, -1.1815],\n",
       "        [-1.3136, -0.8567, -1.1822],\n",
       "        [-1.3137, -0.8566, -1.1823],\n",
       "        [-1.3135, -0.8570, -1.1820],\n",
       "        [-1.3136, -0.8568, -1.1821],\n",
       "        [-1.3135, -0.8568, -1.1822],\n",
       "        [-1.3144, -0.8575, -1.1804],\n",
       "        [-1.3131, -0.8572, -1.1821],\n",
       "        [-1.3137, -0.8570, -1.1817],\n",
       "        [-1.3138, -0.8575, -1.1810],\n",
       "        [-1.3140, -0.8566, -1.1821],\n",
       "        [-1.3133, -0.8574, -1.1815],\n",
       "        [-1.3129, -0.8573, -1.1820],\n",
       "        [-1.3134, -0.8564, -1.1829],\n",
       "        [-1.3139, -0.8570, -1.1816],\n",
       "        [-1.3134, -0.8565, -1.1827],\n",
       "        [-1.3136, -0.8570, -1.1819],\n",
       "        [-1.3134, -0.8573, -1.1816],\n",
       "        [-1.3132, -0.8576, -1.1813],\n",
       "        [-1.3137, -0.8568, -1.1821],\n",
       "        [-1.3137, -0.8575, -1.1810],\n",
       "        [-1.3140, -0.8566, -1.1821],\n",
       "        [-1.3136, -0.8573, -1.1813],\n",
       "        [-1.3134, -0.8576, -1.1812],\n",
       "        [-1.3136, -0.8572, -1.1816],\n",
       "        [-1.3135, -0.8572, -1.1816],\n",
       "        [-1.3135, -0.8572, -1.1816],\n",
       "        [-1.3139, -0.8570, -1.1816],\n",
       "        [-1.3132, -0.8579, -1.1809],\n",
       "        [-1.3132, -0.8577, -1.1812],\n",
       "        [-1.3132, -0.8581, -1.1806],\n",
       "        [-1.3134, -0.8578, -1.1810],\n",
       "        [-1.3135, -0.8576, -1.1811],\n",
       "        [-1.3134, -0.8568, -1.1822],\n",
       "        [-1.3134, -0.8568, -1.1822],\n",
       "        [-1.3133, -0.8575, -1.1813],\n",
       "        [-1.3137, -0.8572, -1.1815],\n",
       "        [-1.3133, -0.8577, -1.1811],\n",
       "        [-1.3134, -0.8575, -1.1814]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing input to GPU\n",
    "tweets = tweets.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "out = model(tweets)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1290, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(out, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01713936573266983"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity code to check if all works\n",
    "train(train_iter, model, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3405"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(train_iter, model, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3826913456728364"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(val_iter, model, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5012601783637068"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_iter, model, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D CNN Performance\n",
    "\n",
    "Based on our initial network we'd like to compare how depth matters vs number of filters in a given layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "LEARNING_RATE=.1\n",
    "MAX_EPOCHS=10\n",
    "\n",
    "def random_search(num_iter):\n",
    "    results = []\n",
    "    for i in range(num_iter):\n",
    "        config = {\n",
    "            #define hyperparameters here\n",
    "            \"layers\": scipy.stats.randint.rvs(1,3),\n",
    "            \"filters\": scipy.stats.randint.rvs(10,200)\n",
    "        }\n",
    "        \n",
    "        print(\"new config\")\n",
    "        print(config)\n",
    "        model = ConvNet(config[\"layers\"],3,config[\"filters\"],nn.ReLU(),output_size=3, VOCAB_SIZE=VOCAB_SIZE, WORD_VEC_SIZE=WORD_VEC_SIZE)\n",
    "        model.to(device)\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    \n",
    "        max_val = 0\n",
    "        best_epoch = 0\n",
    "        for epoch in range(MAX_EPOCHS):\n",
    "        # train the model for one pass over the data\n",
    "            train_loss = train(train_iter,model,criterion,optimizer,device)  \n",
    "        # compute the training accuracy\n",
    "            train_acc = evaluate(train_iter,model,criterion,device)\n",
    "        # compute the validation accuracy\n",
    "            val_acc = evaluate(val_iter,model,criterion,device)\n",
    "            if val_acc > max_val:\n",
    "                max_val = val_acc\n",
    "                best_epoch = epoch+1\n",
    "        # print the loss for every epoch\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))\n",
    "        results.append((max_val,best_epoch,config))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new config\n",
      "{'layers': 1, 'filters': 148}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5155, Validation Accuracy: 0.4222\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5168, Validation Accuracy: 0.4302\n",
      "Epoch [7/10], Loss: 0.0150, Training Accuracy: 0.5258, Validation Accuracy: 0.4452\n",
      "Epoch [8/10], Loss: 0.0149, Training Accuracy: 0.5445, Validation Accuracy: 0.4622\n",
      "Epoch [9/10], Loss: 0.0148, Training Accuracy: 0.5615, Validation Accuracy: 0.4637\n",
      "Epoch [10/10], Loss: 0.0146, Training Accuracy: 0.5727, Validation Accuracy: 0.4642\n",
      "new config\n",
      "{'layers': 1, 'filters': 184}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5155, Validation Accuracy: 0.4212\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5167, Validation Accuracy: 0.4292\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5263, Validation Accuracy: 0.4442\n",
      "Epoch [8/10], Loss: 0.0149, Training Accuracy: 0.5430, Validation Accuracy: 0.4622\n",
      "Epoch [9/10], Loss: 0.0148, Training Accuracy: 0.5600, Validation Accuracy: 0.4682\n",
      "Epoch [10/10], Loss: 0.0146, Training Accuracy: 0.5747, Validation Accuracy: 0.4652\n",
      "new config\n",
      "{'layers': 1, 'filters': 144}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5157, Validation Accuracy: 0.4222\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5180, Validation Accuracy: 0.4277\n",
      "Epoch [8/10], Loss: 0.0150, Training Accuracy: 0.5285, Validation Accuracy: 0.4437\n",
      "Epoch [9/10], Loss: 0.0148, Training Accuracy: 0.5522, Validation Accuracy: 0.4587\n",
      "Epoch [10/10], Loss: 0.0146, Training Accuracy: 0.5743, Validation Accuracy: 0.4667\n",
      "new config\n",
      "{'layers': 1, 'filters': 89}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5157, Validation Accuracy: 0.4232\n",
      "Epoch [8/10], Loss: 0.0150, Training Accuracy: 0.5220, Validation Accuracy: 0.4362\n",
      "Epoch [9/10], Loss: 0.0149, Training Accuracy: 0.5403, Validation Accuracy: 0.4477\n",
      "Epoch [10/10], Loss: 0.0147, Training Accuracy: 0.5598, Validation Accuracy: 0.4632\n",
      "new config\n",
      "{'layers': 1, 'filters': 75}\n",
      "Epoch [1/10], Loss: 0.0158, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5178, Validation Accuracy: 0.4287\n",
      "Epoch [8/10], Loss: 0.0150, Training Accuracy: 0.5247, Validation Accuracy: 0.4432\n",
      "Epoch [9/10], Loss: 0.0148, Training Accuracy: 0.5430, Validation Accuracy: 0.4567\n",
      "Epoch [10/10], Loss: 0.0147, Training Accuracy: 0.5625, Validation Accuracy: 0.4677\n",
      "new config\n",
      "{'layers': 2, 'filters': 173}\n",
      "Epoch [1/10], Loss: 0.0158, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0155, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [8/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [9/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [10/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "new config\n",
      "{'layers': 1, 'filters': 73}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0152, Training Accuracy: 0.5157, Validation Accuracy: 0.4222\n",
      "Epoch [8/10], Loss: 0.0151, Training Accuracy: 0.5183, Validation Accuracy: 0.4302\n",
      "Epoch [9/10], Loss: 0.0150, Training Accuracy: 0.5375, Validation Accuracy: 0.4432\n",
      "Epoch [10/10], Loss: 0.0148, Training Accuracy: 0.5600, Validation Accuracy: 0.4502\n",
      "new config\n",
      "{'layers': 1, 'filters': 188}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0152, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5155, Validation Accuracy: 0.4237\n",
      "Epoch [7/10], Loss: 0.0150, Training Accuracy: 0.5218, Validation Accuracy: 0.4387\n",
      "Epoch [8/10], Loss: 0.0149, Training Accuracy: 0.5363, Validation Accuracy: 0.4557\n",
      "Epoch [9/10], Loss: 0.0148, Training Accuracy: 0.5593, Validation Accuracy: 0.4682\n",
      "Epoch [10/10], Loss: 0.0146, Training Accuracy: 0.5780, Validation Accuracy: 0.4727\n",
      "new config\n",
      "{'layers': 1, 'filters': 197}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5155, Validation Accuracy: 0.4227\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5180, Validation Accuracy: 0.4317\n",
      "Epoch [8/10], Loss: 0.0149, Training Accuracy: 0.5310, Validation Accuracy: 0.4467\n",
      "Epoch [9/10], Loss: 0.0148, Training Accuracy: 0.5537, Validation Accuracy: 0.4652\n",
      "Epoch [10/10], Loss: 0.0146, Training Accuracy: 0.5750, Validation Accuracy: 0.4697\n",
      "new config\n",
      "{'layers': 2, 'filters': 175}\n",
      "Epoch [1/10], Loss: 0.0158, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0155, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [8/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [9/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [10/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "new config\n",
      "{'layers': 1, 'filters': 165}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5155, Validation Accuracy: 0.4222\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5195, Validation Accuracy: 0.4297\n",
      "Epoch [8/10], Loss: 0.0149, Training Accuracy: 0.5315, Validation Accuracy: 0.4557\n",
      "Epoch [9/10], Loss: 0.0148, Training Accuracy: 0.5548, Validation Accuracy: 0.4717\n",
      "Epoch [10/10], Loss: 0.0146, Training Accuracy: 0.5757, Validation Accuracy: 0.4822\n",
      "new config\n",
      "{'layers': 1, 'filters': 110}\n",
      "Epoch [1/10], Loss: 0.0158, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4222\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5167, Validation Accuracy: 0.4282\n",
      "Epoch [7/10], Loss: 0.0150, Training Accuracy: 0.5255, Validation Accuracy: 0.4492\n",
      "Epoch [8/10], Loss: 0.0149, Training Accuracy: 0.5453, Validation Accuracy: 0.4562\n",
      "Epoch [9/10], Loss: 0.0148, Training Accuracy: 0.5615, Validation Accuracy: 0.4512\n",
      "Epoch [10/10], Loss: 0.0146, Training Accuracy: 0.5788, Validation Accuracy: 0.4647\n",
      "new config\n",
      "{'layers': 2, 'filters': 97}\n",
      "Epoch [1/10], Loss: 0.0158, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0155, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0155, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [8/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [9/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [10/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "new config\n",
      "{'layers': 1, 'filters': 99}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5160, Validation Accuracy: 0.4292\n",
      "Epoch [8/10], Loss: 0.0150, Training Accuracy: 0.5238, Validation Accuracy: 0.4367\n",
      "Epoch [9/10], Loss: 0.0149, Training Accuracy: 0.5380, Validation Accuracy: 0.4597\n",
      "Epoch [10/10], Loss: 0.0147, Training Accuracy: 0.5560, Validation Accuracy: 0.4617\n",
      "new config\n",
      "{'layers': 1, 'filters': 104}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5155, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5180, Validation Accuracy: 0.4247\n",
      "Epoch [8/10], Loss: 0.0150, Training Accuracy: 0.5280, Validation Accuracy: 0.4397\n",
      "Epoch [9/10], Loss: 0.0149, Training Accuracy: 0.5475, Validation Accuracy: 0.4597\n",
      "Epoch [10/10], Loss: 0.0147, Training Accuracy: 0.5647, Validation Accuracy: 0.4682\n",
      "new config\n",
      "{'layers': 1, 'filters': 24}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5157, Validation Accuracy: 0.4227\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5190, Validation Accuracy: 0.4257\n",
      "Epoch [8/10], Loss: 0.0150, Training Accuracy: 0.5273, Validation Accuracy: 0.4412\n",
      "Epoch [9/10], Loss: 0.0149, Training Accuracy: 0.5427, Validation Accuracy: 0.4482\n",
      "Epoch [10/10], Loss: 0.0148, Training Accuracy: 0.5547, Validation Accuracy: 0.4557\n",
      "new config\n",
      "{'layers': 1, 'filters': 129}\n",
      "Epoch [1/10], Loss: 0.0157, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5155, Validation Accuracy: 0.4222\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5178, Validation Accuracy: 0.4282\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5260, Validation Accuracy: 0.4422\n",
      "Epoch [8/10], Loss: 0.0149, Training Accuracy: 0.5412, Validation Accuracy: 0.4602\n",
      "Epoch [9/10], Loss: 0.0148, Training Accuracy: 0.5570, Validation Accuracy: 0.4622\n",
      "Epoch [10/10], Loss: 0.0146, Training Accuracy: 0.5720, Validation Accuracy: 0.4607\n",
      "new config\n",
      "{'layers': 2, 'filters': 105}\n",
      "Epoch [1/10], Loss: 0.0158, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0155, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0155, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [8/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [9/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [10/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "new config\n",
      "{'layers': 1, 'filters': 59}\n",
      "Epoch [1/10], Loss: 0.0159, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0153, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0152, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0151, Training Accuracy: 0.5168, Validation Accuracy: 0.4262\n",
      "Epoch [8/10], Loss: 0.0150, Training Accuracy: 0.5295, Validation Accuracy: 0.4502\n",
      "Epoch [9/10], Loss: 0.0148, Training Accuracy: 0.5568, Validation Accuracy: 0.4582\n",
      "Epoch [10/10], Loss: 0.0147, Training Accuracy: 0.5732, Validation Accuracy: 0.4707\n",
      "new config\n",
      "{'layers': 2, 'filters': 147}\n",
      "Epoch [1/10], Loss: 0.0158, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [2/10], Loss: 0.0155, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [3/10], Loss: 0.0155, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [4/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [5/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [6/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [7/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [8/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [9/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n",
      "Epoch [10/10], Loss: 0.0154, Training Accuracy: 0.5157, Validation Accuracy: 0.4217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.464232116058029, 10, {'layers': 1, 'filters': 148}),\n",
       " (0.46823411705852924, 9, {'layers': 1, 'filters': 184}),\n",
       " (0.46673336668334164, 10, {'layers': 1, 'filters': 144}),\n",
       " (0.46323161580790395, 10, {'layers': 1, 'filters': 89}),\n",
       " (0.46773386693346675, 10, {'layers': 1, 'filters': 75}),\n",
       " (0.42171085542771386, 1, {'layers': 2, 'filters': 173}),\n",
       " (0.4502251125562781, 10, {'layers': 1, 'filters': 73}),\n",
       " (0.47273636818409204, 10, {'layers': 1, 'filters': 188}),\n",
       " (0.46973486743371684, 10, {'layers': 1, 'filters': 197}),\n",
       " (0.42171085542771386, 1, {'layers': 2, 'filters': 175}),\n",
       " (0.4822411205602801, 10, {'layers': 1, 'filters': 165}),\n",
       " (0.46473236618309155, 10, {'layers': 1, 'filters': 110}),\n",
       " (0.42171085542771386, 1, {'layers': 2, 'filters': 97}),\n",
       " (0.46173086543271635, 10, {'layers': 1, 'filters': 99}),\n",
       " (0.46823411705852924, 10, {'layers': 1, 'filters': 104}),\n",
       " (0.45572786393196596, 10, {'layers': 1, 'filters': 24}),\n",
       " (0.4622311155577789, 9, {'layers': 1, 'filters': 129}),\n",
       " (0.42171085542771386, 1, {'layers': 2, 'filters': 105}),\n",
       " (0.47073536768384194, 10, {'layers': 1, 'filters': 59}),\n",
       " (0.42171085542771386, 1, {'layers': 2, 'filters': 147})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D CNN summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My best score is 48.47% with 2 layers and 73 filters. Based on the results having two layers gave a better score and having filters in the range of 50-80 gave the best results, anything higher reduced the scores possibly due to ovrefitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use a corpus from the [CL-Aff shared task](https://sites.google.com/view/affcon2019/cl-aff-shared-task?authuser=0). HappyDB is a dataset of about 100,000 `happy moments` crowd-sourced via Amazon’s Mechanical Turk where each worker was asked to describe in a complete sentence `what made them happy in the past 24 hours`. Each user was asked to describe three such moments. \n",
    "\n",
    "\n",
    "We have already preprocessed (tokenization, removing URLs, mentions, hashtags and so on) the tweets and placed it under ``data/happy_db`` folder in three files as ``train.tsv``, ``dev.tsv`` and ``test.tsv``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whitespace tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \n",
    "    # your code goes here \n",
    "    tokens = WhitespaceTokenizer().tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TorchText's Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "TEXT = Field(sequential=True, tokenize=whitespace_tokenize, lower=False)\n",
    "LABEL = Field(sequential=False, unk_token = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TabularDataset class`  and `Fields`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "train,val,test = TabularDataset.splits(path= \"./data/happy_db/\",train='train.tsv', validation=\"dev.tsv\", test=\"test.tsv\", # file names\n",
    "    format='tsv',\n",
    "    skip_header=True, # if your tsv file has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "    fields=[('tweet', TEXT), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "TEXT.build_vocab(train, max_size=5000) # builds vocabulary based on all the words that occur at least twice in the training set\n",
    "LABEL.build_vocab(train, max_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5002\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# your code goes here \n",
    "print(len(TEXT.vocab.stoi))\n",
    "print(len(LABEL.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "#from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    " (train, val, test), # we pass in the datasets we want the iterator to draw data from\n",
    " batch_sizes=(32,32,32),\n",
    " sort_key=lambda x: len(x.tweet), \n",
    " sort=True,\n",
    "# A key to use for sorting examples in order to batch together examples with similar lengths and minimize padding. \n",
    " sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "class LSTMmodel(nn.Module):\n",
    "  \n",
    "  def __init__(self, embedding_size, vocab_size, output_size, hidden_size, num_layers):\n",
    "    # In the constructor we define the layers for our model\n",
    "    super(LSTMmodel, self).__init__()\n",
    "    # word embedding lookup table\n",
    "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=True)\n",
    "    # core LSTM module\n",
    "    self.LSTM_layer = nn.LSTM(input_size=300, hidden_size=500, num_layers=2) \n",
    "    # activation function\n",
    "    self.activation_fn = nn.Tanh()\n",
    "    # classification related modules\n",
    "    self.linear_layer = nn.Linear(hidden_size, output_size) \n",
    "    self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "    self.debug = False\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # In the forward function we define the forward propagation logic\n",
    "    if self.debug:\n",
    "        print(\"input word indices shape = \", x.size())\n",
    "    out = self.embedding(x)\n",
    "    if self.debug:\n",
    "        print(\"word embeddings shape = \", out.size())\n",
    "    out, _ = self.LSTM_layer(out) # since we are not feeding h_0 explicitly, h_0 will be initialized to zeros by default\n",
    "    if self.debug:\n",
    "        print(\"RNN output (features from last layer of RNN for all timesteps) shape = \", out.size())\n",
    "    # classify based on the hidden representation after RNN processes the last token\n",
    "    out = out[-1]\n",
    "    if self.debug:\n",
    "        print(\"Tweet embeddings or RNN output (features from last layer of RNN for the last timestep only) shape = \", out.size())\n",
    "    out = self.activation_fn(out)\n",
    "    if self.debug:\n",
    "        print(\"ReLU output shape = \", out.size())\n",
    "    out = self.linear_layer(out)\n",
    "    if self.debug:\n",
    "        print(\"linear layer output shape = \", out.size())\n",
    "    out = self.softmax_layer(out) # accepts 2D or more dimensional inputs\n",
    "    if self.debug:\n",
    "        print(\"softmax layer output shape = \", out.size())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "HIDDEN_SIZE = 500 # no. of units in the hidden layer\n",
    "NUM_LAYERS = 2\n",
    "EMBEDDING_SIZE = 300\n",
    "VOCAB_SIZE = len(TEXT.vocab.stoi)\n",
    "NUM_CLASSES = len(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMmodel(\n",
      "  (embedding): Embedding(5002, 300, sparse=True)\n",
      "  (LSTM_layer): LSTM(300, 500, num_layers=2)\n",
      "  (activation_fn): Tanh()\n",
      "  (linear_layer): Linear(in_features=500, out_features=2, bias=True)\n",
      "  (softmax_layer): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTMmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS) \n",
    "print(model)\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an optimizer for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "criterion = nn.NLLLoss()\n",
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 5109602\n"
     ]
    }
   ],
   "source": [
    "# your code goes here \n",
    "# your code goes here \n",
    "total_parameters = 0\n",
    "for variable in model.parameters():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.shape\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim\n",
    "    total_parameters += variable_parameters\n",
    "print(\"Total Parameters:\",total_parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Mega Byte memory: 20.440412\n"
     ]
    }
   ],
   "source": [
    "points = 5110103\n",
    "bits_per_point = 32\n",
    "number_of_mega_byte= (((points*bits_per_point)/8)/10**6)\n",
    "print(\"Model Mega Byte memory:\",number_of_mega_byte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(loader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_sample = 0\n",
    "    for batch in loader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.tweet\n",
    "        batch_output = batch.label\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "        # forward propagation\n",
    "        # pass the data through the model\n",
    "        model_outputs = model(batch_input)\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "    return total_loss/num_sample\n",
    "\n",
    "# evaluation logic based on classification accuracy\n",
    "def evaluate(loader):\n",
    "    all_pred=[]\n",
    "    all_label = []\n",
    "    with torch.no_grad(): # impacts the autograd engine and deactivate it. reduces memory usage and speeds up computation\n",
    "        for batch in loader:\n",
    "             # load the current batch\n",
    "            batch_input = batch.tweet\n",
    "            batch_output = batch.label\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            # forward propagation\n",
    "            # pass the data through the model\n",
    "            model_outputs = model(batch_input)\n",
    "            # identify the predicted class for each example in the batch\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            # put all the true labels and predictions to two lists\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output)\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    f1score = f1_score(all_label, all_pred, average='macro') \n",
    "    return accuracy,f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here \n",
    "# Print model's state_dict\n",
    "import os\n",
    "if not os.path.exists(\"./ckpt\"): # check if the directory doesn't exist already\n",
    "    os.mkdir(\"./ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 loss  0.019151628931109426 Train Accuracy & F1 (0.7309422348484849, 0.7137388123133535) Validation Accuracy & F1  (0.6732954545454546, 0.6316692464243143)\n",
      "epoch  2 loss  0.013246820703374617 Train Accuracy & F1 (0.8325047348484849, 0.8324948186057661) Validation Accuracy & F1  (0.7926136363636364, 0.7924571141394334)\n",
      "epoch  3 loss  0.011463872385458231 Train Accuracy & F1 (0.7906013257575758, 0.7885851163107714) Validation Accuracy & F1  (0.7679924242424242, 0.7676421588658725)\n",
      "epoch  4 loss  0.010660828917576564 Train Accuracy & F1 (0.8290719696969697, 0.8284218987839711) Validation Accuracy & F1  (0.7897727272727273, 0.7894396482038348)\n",
      "epoch  5 loss  0.009733532781176495 Train Accuracy & F1 (0.8325047348484849, 0.8315165046642996) Validation Accuracy & F1  (0.7878787878787878, 0.7867151404983717)\n",
      "epoch  6 loss  0.008951072622122329 Train Accuracy & F1 (0.8135653409090909, 0.8113326949609299) Validation Accuracy & F1  (0.7471590909090909, 0.7425695386005999)\n",
      "epoch  7 loss  0.008422675708337038 Train Accuracy & F1 (0.8693181818181818, 0.8691640062979097) Validation Accuracy & F1  (0.8210227272727273, 0.8210225667744264)\n",
      "epoch  8 loss  0.007800204066425618 Train Accuracy & F1 (0.8938210227272727, 0.8937999504810787) Validation Accuracy & F1  (0.8248106060606061, 0.8246783840903891)\n",
      "epoch  9 loss  0.0071551693174776365 Train Accuracy & F1 (0.8894412878787878, 0.889384151593454) Validation Accuracy & F1  (0.8172348484848485, 0.816931303428496)\n",
      "epoch  10 loss  0.0065436911794346415 Train Accuracy & F1 (0.8776041666666666, 0.8752106220183131) Validation Accuracy & F1  (0.8418560606060606, 0.8366226150275844)\n"
     ]
    }
   ],
   "source": [
    "# start the training\n",
    "MAX_EPOCHS = 10\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # train the model for one pass over the data\n",
    "    train_loss = train(train_iter)  \n",
    "    # compute the training accuracy\n",
    "    train_acc = evaluate(train_iter)\n",
    "    # compute the validation accuracy\n",
    "    val_acc = evaluate(val_iter)\n",
    "    \n",
    "    # print the loss for every epoch\n",
    "    print('epoch ',epoch+1,'loss ', train_loss,'Train Accuracy & F1',train_acc,'Validation Accuracy & F1 ', val_acc)\n",
    "    \n",
    "    # save model, optimizer, and number of epoch to a dictionary\n",
    "    model_save = {\n",
    "            'epoch': epoch,  # number of epoch\n",
    "            'model_state_dict': model.state_dict(), # model parameters \n",
    "            'optimizer_state_dict': optimizer.state_dict(), # save optimizer \n",
    "            'loss': train_loss # training loss\n",
    "            }\n",
    "    \n",
    "    # use torch.save to store \n",
    "    torch.save(model_save, \"./ckpt/model_{}.pt\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMmodel(\n",
      "  (embedding): Embedding(5002, 300, sparse=True)\n",
      "  (LSTM_layer): LSTM(300, 500, num_layers=2)\n",
      "  (activation_fn): Tanh()\n",
      "  (linear_layer): Linear(in_features=500, out_features=2, bias=True)\n",
      "  (softmax_layer): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define a new model\n",
    "\n",
    "model2 = LSTMmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS) \n",
    "\n",
    "# load checkpoint\n",
    "\n",
    "checkpoint = torch.load(\"./ckpt/model_9.pt\")\n",
    "\n",
    "# assign the parameters of checkpoint to this new model\n",
    "\n",
    "model2.load_state_dict(checkpoint['model_state_dict'])\n",
    "model2.to(device)\n",
    "\n",
    "print(model2) # can be used for inference or for further training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy :  0.8314393939393939\n",
      "Test F1 :  0.8265100807419248\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "accuracy, f1score = evaluate(test_iter)\n",
    "print(\"Test Accuracy : \", accuracy)\n",
    "print(\"Test F1 : \", f1score)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
